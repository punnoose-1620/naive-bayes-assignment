{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Assignment 1\n",
    "\n",
    "## Naive Bayes Learning algorithm, Cross-validation, and ROC-Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the assignment is to implement:\n",
    "\n",
    "* Naive Bayes learning algorithm for binary classification tasks\n",
    "* Visualization to plot a ROC-curve\n",
    "* A cross-validation test\n",
    "* Visualization of the average ROC-curve of a cross-validation test\n",
    "\n",
    "Follow the instructions and implement what is missing to complete the assignment. Some functions have been started to help you a little bit with the inputs or outputs of the function.\n",
    "\n",
    "**Note:** You might need to go back and forth during your implementation of the code. The structure is set up to make implementation easier, but how you return values from the different functions might vary, and you might find yourself going back and change something to make it easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We help you out with importing the libraries and reading the data.\n",
    "\n",
    "Look at the output to get an idea of how the data is structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import e, pi, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./iris.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped(['Iris-setosa' 'Iris-versicolor' 'Iris-virginica'])\n",
      "index([  0  50 100])\n",
      "unique_arr([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2])\n",
      "Data Before : ['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n",
      "Data After : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Iris Setosa : 50\n",
      "Iris Versicolor : 50\n",
      "Iris Virginica : 50\n"
     ]
    }
   ],
   "source": [
    "mapped, index, unique_arr = np.unique(data[:, -1], return_index=True, return_inverse=True)\n",
    "print(f\"Mapped({mapped})\\nindex({index})\\nunique_arr({unique_arr})\")\n",
    "print(f\"Data Before : {data[:,-1]}\")\n",
    "data[:, -1] = unique_arr\n",
    "print(f\"Data After : {data[:,-1]}\")\n",
    "\n",
    "filtered_data_setVsVir = data[data[:, -1] != 1]\n",
    "filtered_data_setVsVer = data[data[:, -1] != 2]\n",
    "filtered_data_verVsVir = data[data[:, -1] != 0]\n",
    "# print(\"Filtered Data : \", filtered_data)\n",
    "# print(f\"Test : {index[1:]} : {data}\")\n",
    "iris_setosa, iris_versicolor, iris_virginica = np.split(data, index[1:])\n",
    "print(f\"Iris Setosa : {len(iris_setosa)}\\nIris Versicolor : {len(iris_versicolor)}\\nIris Virginica : {len(iris_virginica)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print trial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data array (features and labels):\n",
      "[[5.1 3.5 1.4 0.2 0]\n",
      " [4.9 3.0 1.4 0.2 0]\n",
      " [4.7 3.2 1.3 0.2 0]]\n",
      "\n",
      "###############\n",
      "\n",
      "Train features (first 4 columns):\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.0 1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]]\n",
      "\n",
      "Labels (last column):\n",
      "[[0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "Names of labels:\n",
      "[[0, 'Iris-setosa'], [1, 'Iris-versicolor'], [2, 'Iris-virginica']]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Full data array (features and labels):\\n{iris_setosa[:3]}\\n\")\n",
    "print(\"###############\\n\")\n",
    "print(f\"Train features (first 4 columns):\\n{iris_setosa[:3, :-1]}\\n\")\n",
    "print(f\"Labels (last column):\\n{iris_setosa[:3, -1:]}\\n\")\n",
    "print(f\"Names of labels:\\n{[[numb, name] for numb, name in enumerate(mapped)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling functions\n",
    "\n",
    "As a start, we are going to implement some basic data handling functions to use in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Split class into a train and test set\n",
    "\n",
    "First, we need to be able to split the class into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio=0.8\n",
    "def train_test_split(class_data):\n",
    "\n",
    "    class_data = np.array(class_data)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    features = class_data[:, :-1]\n",
    "    labels = class_data[:, -1]\n",
    "    # print(f\"Train Test Split Labels : {labels}\")\n",
    "    \n",
    "    unique_classes = np.unique(labels)\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(labels == cls)[0]\n",
    "        np.random.shuffle(cls_indices)\n",
    "        split_index = int(len(cls_indices) * split_ratio)\n",
    "        train_indices = cls_indices[:split_index]\n",
    "        test_indices = cls_indices[split_index:]\n",
    "        train_data.extend(class_data[train_indices])\n",
    "        test_data.extend(class_data[test_indices])\n",
    "    \n",
    "    train_data = np.array(train_data)\n",
    "    test_data = np.array(test_data)\n",
    "    \n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(test_data)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data :  [[4.8 3.1 1.6 0.2 0]\n",
      " [5.1 3.8 1.6 0.2 0]\n",
      " [5.1 3.8 1.9 0.4 0]\n",
      " [5.9 3.2 4.8 1.8 1]\n",
      " [4.9 2.4 3.3 1.0 1]\n",
      " [5.8 2.7 3.9 1.2 1]\n",
      " [5.0 3.5 1.3 0.3 0]\n",
      " [5.8 2.7 4.1 1.0 1]\n",
      " [4.8 3.0 1.4 0.1 0]\n",
      " [5.7 2.9 4.2 1.3 1]\n",
      " [4.9 3.1 1.5 0.1 0]\n",
      " [5.7 2.8 4.1 1.3 1]\n",
      " [5.5 2.5 4.0 1.3 1]\n",
      " [6.4 2.9 4.3 1.3 1]\n",
      " [6.2 2.2 4.5 1.5 1]\n",
      " [5.0 3.6 1.4 0.2 0]\n",
      " [6.5 2.8 4.6 1.5 1]\n",
      " [5.0 3.3 1.4 0.2 0]\n",
      " [5.4 3.4 1.5 0.4 0]\n",
      " [5.4 3.4 1.7 0.2 0]\n",
      " [6.6 2.9 4.6 1.3 1]\n",
      " [5.6 2.7 4.2 1.3 1]\n",
      " [5.6 3.0 4.5 1.5 1]\n",
      " [5.8 2.6 4.0 1.2 1]\n",
      " [6.3 3.3 4.7 1.6 1]\n",
      " [5.4 3.9 1.3 0.4 0]\n",
      " [4.5 2.3 1.3 0.3 0]\n",
      " [5.5 2.4 3.7 1.0 1]\n",
      " [5.0 3.4 1.6 0.4 0]\n",
      " [5.7 3.8 1.7 0.3 0]\n",
      " [4.7 3.2 1.6 0.2 0]\n",
      " [5.2 4.1 1.5 0.1 0]\n",
      " [5.6 3.0 4.1 1.3 1]\n",
      " [4.6 3.2 1.4 0.2 0]\n",
      " [5.0 3.2 1.2 0.2 0]\n",
      " [4.9 3.0 1.4 0.2 0]\n",
      " [4.8 3.4 1.6 0.2 0]\n",
      " [6.1 2.9 4.7 1.4 1]\n",
      " [5.7 2.6 3.5 1.0 1]\n",
      " [5.2 3.5 1.5 0.2 0]\n",
      " [6.6 3.0 4.4 1.4 1]\n",
      " [5.7 2.8 4.5 1.3 1]\n",
      " [4.3 3.0 1.1 0.1 0]\n",
      " [5.6 2.5 3.9 1.1 1]\n",
      " [4.4 2.9 1.4 0.2 0]\n",
      " [5.5 4.2 1.4 0.2 0]\n",
      " [5.3 3.7 1.5 0.2 0]\n",
      " [6.0 2.9 4.5 1.5 1]\n",
      " [5.1 3.7 1.5 0.4 0]\n",
      " [4.4 3.0 1.3 0.2 0]\n",
      " [6.9 3.1 4.9 1.5 1]\n",
      " [4.6 3.6 1.0 0.2 0]\n",
      " [5.5 2.6 4.4 1.2 1]\n",
      " [6.3 2.3 4.4 1.3 1]\n",
      " [6.7 3.1 4.7 1.5 1]\n",
      " [6.8 2.8 4.8 1.4 1]\n",
      " [5.4 3.7 1.5 0.2 0]\n",
      " [5.7 4.4 1.5 0.4 0]\n",
      " [7.0 3.2 4.7 1.4 1]\n",
      " [6.1 2.8 4.0 1.3 1]\n",
      " [5.1 3.5 1.4 0.2 0]\n",
      " [4.7 3.2 1.3 0.2 0]\n",
      " [6.3 2.5 4.9 1.5 1]\n",
      " [4.6 3.4 1.4 0.3 0]\n",
      " [6.0 2.7 5.1 1.6 1]\n",
      " [5.0 3.4 1.5 0.2 0]\n",
      " [6.0 3.4 4.5 1.6 1]\n",
      " [5.1 2.5 3.0 1.1 1]\n",
      " [6.7 3.1 4.4 1.4 1]\n",
      " [4.9 3.1 1.5 0.1 0]\n",
      " [5.1 3.5 1.4 0.3 0]\n",
      " [5.1 3.3 1.7 0.5 0]\n",
      " [5.5 2.3 4.0 1.3 1]\n",
      " [5.4 3.9 1.7 0.4 0]\n",
      " [5.2 2.7 3.9 1.4 1]\n",
      " [5.1 3.4 1.5 0.2 0]\n",
      " [6.4 3.2 4.5 1.5 1]\n",
      " [5.0 2.3 3.3 1.0 1]\n",
      " [4.9 3.1 1.5 0.1 0]\n",
      " [5.9 3.0 4.2 1.5 1]]\n",
      "Test Data :  [[4.6 3.1 1.5 0.2 0]\n",
      " [5.2 3.4 1.4 0.2 0]\n",
      " [6.0 2.2 4.0 1.0 1]\n",
      " [5.5 3.5 1.3 0.2 0]\n",
      " [5.0 2.0 3.5 1.0 1]\n",
      " [4.8 3.4 1.9 0.2 0]\n",
      " [5.4 3.0 4.5 1.5 1]\n",
      " [5.0 3.0 1.6 0.2 0]\n",
      " [5.1 3.8 1.5 0.3 0]\n",
      " [5.0 3.5 1.6 0.6 0]\n",
      " [5.6 2.9 3.6 1.3 1]\n",
      " [4.4 3.2 1.3 0.2 0]\n",
      " [6.1 3.0 4.6 1.4 1]\n",
      " [4.8 3.0 1.4 0.3 0]\n",
      " [5.8 4.0 1.2 0.2 0]\n",
      " [6.2 2.9 4.3 1.3 1]\n",
      " [5.7 3.0 4.2 1.2 1]\n",
      " [6.7 3.0 5.0 1.7 1]\n",
      " [6.1 2.8 4.7 1.2 1]\n",
      " [5.5 2.4 3.8 1.1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Test the train_test_split function\n",
    "train_data_setVsVer, test_data_setVsVer = train_test_split(filtered_data_setVsVer)\n",
    "train_data_setVsVir, test_data_setVsVir = train_test_split(filtered_data_setVsVir)\n",
    "train_data_verVsVir, test_data_verVsVir = train_test_split(filtered_data_verVsVir)\n",
    "# print(f\"Test : {train_data[:,-1]}\")\n",
    "\n",
    "# Print the output\n",
    "# print(\"Train Data : \",len(train_data_setVsVer))\n",
    "# print(\"Test Data : \",len(test_data_setVsVer))\n",
    "\n",
    "print(\"Train Data : \",train_data_setVsVer)\n",
    "print(\"Test Data : \",test_data_setVsVer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split data into features and labels\n",
    "\n",
    "The data as shown above is not always the optimal shape. To help us keep track of things, we can split the data into its features and labels seperately.\n",
    "\n",
    "Each class is 4 features and 1 label in the same array: \n",
    "\n",
    "* **[feature 1, feature 2, feature 3, feature 4, label]**\n",
    "\n",
    "It would help us later to have the features and labels in seperate arrays in the form: \n",
    "\n",
    "* **[feature 1, feature 2, feature 3, feature 4]** and **[label]**\n",
    "\n",
    "Here you are going to implement this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_split(data):\n",
    "    features = data[:, :-1].astype(float)\n",
    "    labels = data[:, -1].astype(int)\n",
    "    # print(f\"Class Split Debug \\nFeatures : {len(features)}\\nLabels : \\n{labels}\")\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should first test the \"**class_split**\" function on one of the classes above (iris_setosa, etc...) to make sure it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Features :  [[6.1 3.  4.9 1.8]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.  3.  4.8 1.8]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [7.2 3.  5.8 1.6]]\n",
      "Train Data Labels :  [2 2 1 1 2 1 2 1 2 2 2 1 2 1 2 2 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 1 2 2\n",
      " 1 2 1 2 1 1 2 2 1 1 2 1 1 2 1 2 2 2 1 1 2 2 2 2 2 1 1 2 1 2 1 2 1 2 2 1 1\n",
      " 2 2 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Test the class splitting function\n",
    "train_features_setVsVer, train_labels_setVsVer = class_split(train_data_setVsVer)\n",
    "train_features_setVsVir, train_labels_setVsVir = class_split(train_data_setVsVir)\n",
    "train_features_verVsVir, train_labels_verVsVir = class_split(train_data_verVsVir)\n",
    "\n",
    "# Print the output\n",
    "print(\"Train Data Features : \",train_features_verVsVir)\n",
    "print(\"Train Data Labels : \",train_labels_verVsVir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also try to **1)** first split a class into a train and test set, **2)** split each of these two into features and abels. In total there should be 4 arrays (2 feature and 2 label arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Features :  [[6.1 3.  4.9 1.8]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.7 3.  5.  1.7]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.  3.  4.8 1.8]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [7.2 3.  5.8 1.6]]\n",
      "Train Data Labels :  [2 2 1 1 2 1 2 1 2 2 2 1 2 1 2 2 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 1 2 2\n",
      " 1 2 1 2 1 1 2 2 1 1 2 1 1 2 1 2 2 2 1 1 2 2 2 2 2 1 1 2 1 2 1 2 1 2 2 1 1\n",
      " 2 2 1 1 2 2]\n",
      "Test Data Features :  [[5.2 2.7 3.9 1.4]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.7 3.1 4.4 1.4]]\n",
      "Test Data Labels :  [1 2 2 1 2 1 2 1 2 2 1 2 2 2 1 1 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Use the above data handling functions to make 2 feature arrays and 2 label arrays. \n",
    "train_features_setVsVer, train_labels_setVsVer = class_split(train_data_setVsVer)\n",
    "test_features_setVsVer, test_labels_setVsVer = class_split(test_data_setVsVer)\n",
    "\n",
    "train_features_setVsVir, train_labels_setVsVir = class_split(train_data_setVsVir)\n",
    "test_features_setVsVir, test_labels_setVsVir = class_split(test_data_setVsVir)\n",
    "\n",
    "train_features_verVsVir, train_labels_verVsVir = class_split(train_data_verVsVir)\n",
    "test_features_verVsVir, test_labels_verVsVir = class_split(test_data_verVsVir)\n",
    "\n",
    "\n",
    "# TODO: Print the output (you may use multiple print-statements if you wish)\n",
    "print(\"Train Data Features : \",train_features_verVsVir)\n",
    "print(\"Train Data Labels : \",train_labels_verVsVir)\n",
    "print(\"Test Data Features : \",test_features_verVsVir)\n",
    "print(\"Test Data Labels : \",test_labels_verVsVir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think a bit before going to the next task, what can easily go wrong in the above code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing the Navie Bayes learning algorithm, we can break it down into a few components.\n",
    "\n",
    "We will implement these components one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Calculate feature statistics\n",
    "\n",
    "First, we need to implement a function that returns feature statistics (means, standard deviation, priors) for a given set of feature data for a single class. This is the equivalent of \"training\" the naive bayes model.\n",
    "\n",
    "**Note 1:** Each feature gets its own mean and standard deviation!\n",
    "\n",
    "**Note 2:** The way you structure the functions (what is returned) shapes the remainder of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_statistics(feature_data, class_labels):\n",
    "    stats = {}\n",
    "    unique_labels = np.unique(class_labels)\n",
    "    for label in unique_labels:\n",
    "        class_data = feature_data[class_labels == label].astype(float)\n",
    "        stats[label] = {\n",
    "            'mean': np.mean(class_data, axis=0),\n",
    "            'std': np.sqrt(np.var(class_data, axis=0)),\n",
    "            'prior': len(class_data) / len(feature_data)\n",
    "        }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the function works, we should test it before proceding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setosa Feature Statistics :  {'mean': array([5.9575, 2.7775, 4.27  , 1.34  , 1.    ]), 'std': array([0.5113157 , 0.33278184, 0.48228622, 0.19849433, 0.        ]), 'prior': 0.5}\n",
      "Versicolor Feature Statistics :  {'mean': array([5.9575, 2.7775, 4.27  , 1.34  , 1.    ]), 'std': array([0.5113157 , 0.33278184, 0.48228622, 0.19849433, 0.        ]), 'prior': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Make sure to use our previous class splitting function.\n",
    "feature_stats_temp_setVsVer = calculate_feature_statistics(train_data_setVsVer, train_labels_setVsVer)\n",
    "feature_stats_temp_setVsVir = calculate_feature_statistics(train_data_setVsVir, train_labels_setVsVir)\n",
    "feature_stats_temp_verVsVir = calculate_feature_statistics(train_data_verVsVir, train_labels_verVsVir)\n",
    "\n",
    "print(\"Setosa Feature Statistics : \",feature_stats_temp_verVsVir[1]) \n",
    "# print(\"Setosa vs Virginica Feature Statistics : \",feature_stats_temp_setVsVir) \n",
    "print(\"Versicolor Feature Statistics : \",feature_stats_temp_verVsVir[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Gaussian probability density function (Gaussian PDF)\n",
    "\n",
    "Now we need to implement the gaussian probability density function to use for a single datapoint.\n",
    "\n",
    "**Note:** Look at the imports in the first cell at the top, it has some math numbers for easy use here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_probability_density_function(x, mean, std):\n",
    "    x = np.array(x)\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "    exponent = np.exp(-((x - mean) ** 2 / (2 * std ** 2)))\n",
    "    return (1 / (np.sqrt(2 * np.pi) * std)) * exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Naive Bayes for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Prepare the data for inference\n",
    "\n",
    "Before we train and test the naive bayes for multiple classes, we should get our data in order.\n",
    "\n",
    "Similar to how we did previously, we should now split two classes into a train and test set, you may choose which two classes freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_setVsVer = []\n",
    "true_labels_setVsVer = []\n",
    "\n",
    "predictions_setVsVir = []\n",
    "true_labels_setVsVir = []\n",
    "\n",
    "predictions_verVsVir = []\n",
    "true_labels_verVsVir = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Class A vs Class B for binary classification\n",
    "\n",
    "**Note:** You might need to go back and forth a bit in the following cells during your implementation of your code.\n",
    "\n",
    "We have to get the probability from two sets of classes and compare the two probabilities in order to make a propper prediction.\n",
    "\n",
    "Here we will implement two functions to make this possible. We seperate these functions to make the implementation of the ROC-curve easier later on.\n",
    "\n",
    "**Function 1: naive_bayes_prediction** \n",
    "* A function that returns the probabilities for each class the model for a single datapoint.\n",
    "\n",
    "**Function 2: probabilities_to_prediction**\n",
    "* A function that takes in probabilities and returns a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_prediction(feature_stats, data_point):\n",
    "    probabilities = {}\n",
    "    exes = []\n",
    "    probs = []\n",
    "    total_probability = 0\n",
    "    \n",
    "    for class_label, stats in feature_stats.items():\n",
    "        log_likelihood = 0\n",
    "        for i, feature_value in enumerate(data_point):\n",
    "            mean = stats['mean'][i]\n",
    "            std_dev = stats['std'][i]\n",
    "            exes.append(feature_value)\n",
    "            # print(f\"Naive Bayes Test : mean({mean}) : std_dev({std_dev})\")\n",
    "            prob_den = gaussian_probability_density_function(feature_value, mean, std_dev)\n",
    "            probs.append(prob_den)\n",
    "            log_likelihood += np.log(gaussian_probability_density_function(feature_value, mean, std_dev))\n",
    "        log_prior = np.log(stats['prior'])\n",
    "        probabilities[class_label] = np.exp(log_likelihood + log_prior)\n",
    "        total_probability += probabilities[class_label]\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    for class_label in probabilities:\n",
    "        probabilities[class_label] /= total_probability\n",
    "    \n",
    "    return probabilities, exes, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities_to_prediction(probabilities):\n",
    "    class_prediction = max(probabilities, key=probabilities.get)\n",
    "    return class_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Information : [[0, 'Iris-setosa'], [1, 'Iris-versicolor'], [2, 'Iris-virginica']]\n",
      "Class Labels : [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "labels_info = [[numb, name] for numb, name in enumerate(mapped)]\n",
    "class_labels = [label[0] for label in labels_info]\n",
    "print(f\"Label Information : {labels_info}\")\n",
    "print(f\"Class Labels : {class_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-curve\n",
    "\n",
    "A ROC curve, or *Receiver Operating Characteristic curve*, is a graphical plot that illustrates the performance of a binary classifier such as our Naive Bayes model.\n",
    "\n",
    "More info can be found in the course material and here: [https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "Another good illustration by Google can be found here: [https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n",
    "Now that we have a prediction model, we would want to try it out and test it using a ROC-curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "\n",
    "From our prediction function we get probabilities, and for prediction purposes we have just predicted the one with the highest probability.\n",
    "\n",
    "To plot a ROC-curve, we need the TPR and FPR for the binary classification. We will implement this here.\n",
    "\n",
    "**Note 1:** The threshold is is a value that goes from 0 to 1. \n",
    "\n",
    "**Note 2:** One of the two classes will be seen as \"the positive class\" (prediction over the threshold) and the other as \"the negative class\" (prediction under the threshold).\n",
    "\n",
    "**Note 3:** The threshold stepsize will decide the size of the returned TPR/FPR list. A value of 0.1 will give 10 elements (0 to 1 in increments of 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      "Numpy linspace: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "Numpy linspace (no endpoint): [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "# Stepsize demonstration\n",
    "print(\"Python list:\", [x/10 for x in range(0,10,1)])\n",
    "\n",
    "# Stepsize demonstration with numpy:\n",
    "print(\"Numpy linspace:\", np.linspace(0,1,11))\n",
    "print(\"Numpy linspace (no endpoint):\", np.linspace(0,1,10,endpoint=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPR_and_FPR(prediction_probabilities, test_labels, threshold_stepsize = 0.1):\n",
    "    # Define thresholds\n",
    "    thresholds = np.linspace(0, 1, int(1 / threshold_stepsize) + 1)\n",
    "\n",
    "    # Initialize TPR and FPR lists\n",
    "    TPR, FPR = [], []\n",
    "\n",
    "    # Ensure binary labels\n",
    "    unique_labels = np.unique(test_labels)\n",
    "    print(f\"TPR_FPR Unique Classes Debug : {unique_labels} : {len(unique_labels)}\")\n",
    "    if len(unique_labels) != 2:\n",
    "        raise ValueError(\"The labels must be binary (two unique classes).\")\n",
    "    positive_class = unique_labels[1]  # Higher value as positive class\n",
    "    true_binary = np.array([1 if label == positive_class else 0 for label in test_labels])\n",
    "\n",
    "    # Iterate through thresholds\n",
    "    for threshold in thresholds:\n",
    "        # Predicted positives based on the threshold\n",
    "        predicted_positive = (np.array(prediction_probabilities) >= threshold).astype(int)\n",
    "\n",
    "        # Confusion matrix components\n",
    "        TP = np.sum((predicted_positive == 1) & (true_binary == 1))\n",
    "        FP = np.sum((predicted_positive == 1) & (true_binary == 0))\n",
    "        FN = np.sum((predicted_positive == 0) & (true_binary == 1))\n",
    "        TN = np.sum((predicted_positive == 0) & (true_binary == 0))\n",
    "\n",
    "        # Calculate TPR and FPR\n",
    "        tprs = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        fprs = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "        # Append results\n",
    "        TPR.append(tprs)\n",
    "        FPR.append(fprs)\n",
    "\n",
    "    return np.array(TPR), np.array(FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the values change if you change the threshold stepsize? \n",
    "\n",
    "How does the values change if you change the classes you compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Plot the TPR and FPR\n",
    "\n",
    "To better see what is going on, we can plot the TPR and FPR. We can also calculate the Area Under the ROC Curve (AUC or AUROC) at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(TPR, FPR, class_1, class_2):\n",
    "    auc_score = np.trapz(sorted(TPR), sorted(FPR))\n",
    "    plt.plot(FPR, TPR, color='b', label=f\"AUC = {auc_score:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='dashdot', color='r', label='Random Guess')\n",
    "    class_1_title = \"Iris\"\n",
    "    class_2_title = \"Iris\"\n",
    "    if class_1==0:\n",
    "        class_1_title = class_1_title+\"-setosa\"\n",
    "    elif class_1==1:\n",
    "        class_1_title = class_1_title+\"-versicolor\"\n",
    "    else:\n",
    "        class_1_title = class_1_title+\"-virginica\"\n",
    "\n",
    "    if class_2==0:\n",
    "        class_2_title = class_2_title+\"-setosa\"\n",
    "    elif class_2==1:\n",
    "        class_2_title = class_2_title+\"-versicolor\"\n",
    "    else:\n",
    "        class_2_title = class_2_title+\"-virginica\"\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f\"ROC Curve: {class_1_title} vs. {class_2_title}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "The final task is to take everything you have implemented so far and apply it in a cross-validation loop.\n",
    "\n",
    "**Note 1:** To better reflect a real scenarios, you should shuffle the data before doing cross-validation.\n",
    "\n",
    "**Note 2:** When using cross-validation, the interesting thing is the mean performance (mean AUC, mean accuracy, mean ROC-curve).\n",
    "\n",
    "**Note 3:** This part is a bit more free in terms of implementation, but make sure to use some of the previously implemented functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Cross-validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, target, folds = 10, threshold_stepsize = 0.1):\n",
    "    fold_size = len(data) // folds\n",
    "    unique_classes = np.unique(target)\n",
    "    print(f\"Cross Validation Unique Classes Debug : {unique_classes} : {len(unique_classes)}\")\n",
    "    if len(unique_classes) != 2:\n",
    "        raise ValueError(\"ROC analysis requires exactly two classes.\")\n",
    "    \n",
    "    class_1, class_2 = unique_classes[1], unique_classes[0]\n",
    "    # print(f\"Target({target}) : Class 1({class_1}) and Class 2({class_2})\")\n",
    "    all_TPR, all_FPR = [], []\n",
    "\n",
    "    def data_merge_and_split():\n",
    "        class_1_data = []\n",
    "        class_2_data = []\n",
    "        for i in range(data):\n",
    "            if target[i] == unique_classes[0]:\n",
    "                class_1_data.append(data[i])\n",
    "            else:\n",
    "                class_2_data.append(data[i])\n",
    "        return class_1_data, class_2_data\n",
    "    \n",
    "    def get_nth_fold_data(labels, n, fold_size = 5):\n",
    "        class_1_data, class_2_data = data_merge_and_split()\n",
    "        len_1 = len(class_1_data)\n",
    "        len_2 = len(class_2_data)\n",
    "        fold_lim_1 = round(len_1/fold_size)\n",
    "        fold_lim_2 = round(len_2/fold_size)\n",
    "\n",
    "        fold_start_1 = (n*fold_lim_1)\n",
    "        fold_start_2 = (n*fold_lim_2)\n",
    "\n",
    "        fold_end_1 = fold_start_1+fold_lim_1\n",
    "        fold_end_2 = fold_start_2+fold_lim_2\n",
    "\n",
    "        returnData = []\n",
    "        returnLabels = []\n",
    "        for index in range(fold_start_1, fold_end_1+1):            \n",
    "            returnData.append(class_1_data[index])\n",
    "            returnLabels.append(labels[0])\n",
    "        for index in range(fold_start_2, fold_end_2+1):            \n",
    "            returnData.append(class_2_data[index])\n",
    "            returnLabels.append(labels[1])\n",
    "\n",
    "        return np.array(returnData), np.array(returnLabels)\n",
    "\n",
    "    \n",
    "    for i in range(folds):\n",
    "        # Create train-test splits\n",
    "        start, end = i * fold_size, (i + 1) * fold_size\n",
    "        test_data = data[start:end]\n",
    "        test_labels = target[start:end]\n",
    "        train_data = np.concatenate((data[:start], data[end:]), axis=0)\n",
    "        train_labels = np.concatenate((target[:start], target[end:]), axis=0)\n",
    "\n",
    "        data_new, labels_new = get_nth_fold_data(unique_classes, i, folds)\n",
    "        \n",
    "        # Train Naive Bayes and predict probabilities\n",
    "        feature_stats = calculate_feature_statistics(train_data, train_labels)\n",
    "        prediction_probabilities = []\n",
    "        for x in test_data:\n",
    "            prob_temp,t1,t2 = naive_bayes_prediction(feature_stats, x) \n",
    "            prediction_probabilities.append(prob_temp[class_1])\n",
    "        \n",
    "        # Calculate TPR and FPR\n",
    "        TPR, FPR = TPR_and_FPR(prediction_probabilities, test_labels, threshold_stepsize)\n",
    "        if i==0:\n",
    "            print(f\"Sample True Positive Rate : {TPR}\")\n",
    "            print(f\"Sample False Positive Rate : {FPR}\")\n",
    "        all_TPR.append(TPR)\n",
    "        all_FPR.append(FPR)\n",
    "    \n",
    "    # Average TPR and FPR across folds\n",
    "    mean_TPR = np.mean(all_TPR, axis=0)\n",
    "    mean_FPR = np.mean(all_FPR, axis=0)\n",
    "    \n",
    "    # Plot ROC\n",
    "    plot_ROC(mean_TPR, mean_FPR, class_1, class_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) 10-fold Cross-validation on all classes\n",
    "\n",
    "Test the \"cross_validation\" function on all the classes against eachother using 10 folds.\n",
    "\n",
    "* Iris-setosa vs Iris-versicolor\n",
    "* Iris-setosa vs Iris-virginica\n",
    "* Iris-versicolor vs Iris-virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement and test cross-validation function on all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gaussian Probability Density for Versicolor\n",
      "Test Data Points : [4.6, 3.1, 1.5, 0.2, 4.6, 3.1, 1.5, 0.2, 5.2, 3.4, 1.4, 0.2, 5.2, 3.4, 1.4, 0.2, 6.0, 2.2, 4.0, 1.0, 6.0, 2.2, 4.0, 1.0, 5.5, 3.5, 1.3, 0.2, 5.5, 3.5, 1.3, 0.2, 5.0, 2.0, 3.5, 1.0, 5.0, 2.0, 3.5, 1.0, 4.8, 3.4, 1.9, 0.2, 4.8, 3.4, 1.9, 0.2, 5.4, 3.0, 4.5, 1.5, 5.4, 3.0, 4.5, 1.5, 5.0, 3.0, 1.6, 0.2, 5.0, 3.0, 1.6, 0.2, 5.1, 3.8, 1.5, 0.3, 5.1, 3.8, 1.5, 0.3, 5.0, 3.5, 1.6, 0.6, 5.0, 3.5, 1.6, 0.6, 5.6, 2.9, 3.6, 1.3, 5.6, 2.9, 3.6, 1.3, 4.4, 3.2, 1.3, 0.2, 4.4, 3.2, 1.3, 0.2, 6.1, 3.0, 4.6, 1.4, 6.1, 3.0, 4.6, 1.4, 4.8, 3.0, 1.4, 0.3, 4.8, 3.0, 1.4, 0.3, 5.8, 4.0, 1.2, 0.2, 5.8, 4.0, 1.2, 0.2, 6.2, 2.9, 4.3, 1.3, 6.2, 2.9, 4.3, 1.3, 5.7, 3.0, 4.2, 1.2, 5.7, 3.0, 4.2, 1.2, 6.7, 3.0, 5.0, 1.7, 6.7, 3.0, 5.0, 1.7, 6.1, 2.8, 4.7, 1.2, 6.1, 2.8, 4.7, 1.2, 5.5, 2.4, 3.8, 1.1, 5.5, 2.4, 3.8, 1.1]\n",
      "Probability Densities : [0.5803119568880684, 0.722286669691712, 2.3311980470136002, 3.622314283087063, 0.02463068137344643, 0.7584042475934161, 1.5440525693602898e-08, 2.9007596499819727e-08, 0.9965057048263106, 1.01824917319761, 2.2288570946264787, 3.622314283087063, 0.26150964394755394, 0.15392218273934966, 4.161923971180093e-09, 2.9007596499819727e-08, 0.014906929316206276, 0.007537352595285503, 1.4552650290972455e-50, 3.406980959119509e-12, 0.7659885078320375, 0.1954366266401142, 0.7260993129644188, 0.42085900911059587, 0.39850537611283526, 1.0017329255826022, 1.4880232130112887, 3.622314283087063, 0.516678480150023, 0.07205568906239769, 1.0708677752328236e-09, 2.9007596499819727e-08, 1.1828627700463146, 0.0013316631514253105, 1.0061160653828954e-32, 3.406980959119509e-12, 0.13799429335799482, 0.04139282990958748, 0.21679380172166332, 0.42085900911059587, 0.9877831330066644, 1.01824917319761, 0.07687962235009599, 3.622314283087063, 0.06278521211118605, 0.15392218273934966, 1.837512565530246e-06, 2.9007596499819727e-08, 0.5906060418946157, 0.5651683566211246, 2.6536693900350204e-72, 2.779917344574615e-33, 0.42730415260008686, 1.0280183050400589, 0.7606526424463623, 1.474065894828882, 1.1828627700463146, 0.5651683566211246, 1.702552628612289, 3.622314283087063, 0.13799429335799482, 1.0280183050400589, 5.46814056291036e-08, 2.9007596499819727e-08, 1.1344829997414294, 0.6441615194029138, 2.3311980470136002, 3.2902359519901507, 0.19351808844831345, 0.0037365447908548695, 1.5440525693602898e-08, 6.041179396038447e-07, 1.1828627700463146, 1.0017329255826022, 1.702552628612289, 0.007698474954801716, 0.13799429335799482, 0.07205568906239769, 5.46814056291036e-08, 0.0010259148903640103, 0.246257037951847, 0.41422623983147855, 5.580306182379148e-36, 1.355624959664716e-23, 0.6020178089402615, 1.2437059938288049, 0.30298239841574415, 2.0591357089019295, 0.2398468955593272, 0.8646351593990903, 1.4880232130112887, 3.622314283087063, 0.008331406177523055, 0.4993642573095253, 1.0708677752328236e-09, 2.9007596499819727e-08, 0.005935174551813008, 0.5651683566211246, 4.056028937179376e-77, 3.139649328111757e-28, 0.7415488165288765, 1.0280183050400589, 0.6678107278312713, 2.002569653794173, 0.9877831330066644, 0.5651683566211246, 2.2288570946264787, 3.2902359519901507, 0.06278521211118605, 1.0280183050400589, 4.161923971180093e-09, 6.041179396038447e-07, 0.07223573347322332, 0.3460304476569444, 0.6936839029948058, 3.622314283087063, 0.731314838865942, 0.000294276384999063, 2.6301905006449206e-10, 2.9007596499819727e-08, 0.002164197879370605, 0.41422623983147855, 3.867333638606162e-63, 1.355624959664716e-23, 0.6917719427308496, 1.2437059938288049, 0.8583833691812945, 2.0591357089019295, 0.13936742804267788, 0.5651683566211246, 8.614512317900905e-59, 2.2377270981744514e-19, 0.6759335598372731, 1.0280183050400589, 0.8504391099150963, 1.6025401388551526, 3.7315291146712238e-06, 0.5651683566211246, 6.100455298080365e-98, 1.2177575141215163e-44, 0.28032673053299256, 1.0280183050400589, 0.24924010108754163, 0.34630220196816897, 0.005935174551813008, 0.2843733572655755, 4.3289214266591406e-82, 2.2377270981744514e-19, 0.7415488165288765, 1.3429238757956727, 0.5596674534143005, 1.6025401388551526, 0.39850537611283526, 0.032840472891387106, 5.844551689055927e-43, 1.4121597787597972e-15, 0.516678480150023, 0.5855364700259005, 0.5147394064899623, 0.9439727746015536]\n",
      "\n",
      "Testing Gaussian Probability Density for Virginica\n",
      "Test Data Points : [6.2, 3.4, 5.4, 2.3, 6.2, 3.4, 5.4, 2.3, 6.7, 3.1, 5.6, 2.4, 6.7, 3.1, 5.6, 2.4, 6.9, 3.2, 5.7, 2.3, 6.9, 3.2, 5.7, 2.3, 4.6, 3.2, 1.4, 0.2, 4.6, 3.2, 1.4, 0.2, 6.8, 3.0, 5.5, 2.1, 6.8, 3.0, 5.5, 2.1, 5.1, 3.5, 1.4, 0.3, 5.1, 3.5, 1.4, 0.3, 4.5, 2.3, 1.3, 0.3, 4.5, 2.3, 1.3, 0.3, 6.5, 3.0, 5.2, 2.0, 6.5, 3.0, 5.2, 2.0, 5.0, 3.3, 1.4, 0.2, 5.0, 3.3, 1.4, 0.2, 5.4, 3.9, 1.3, 0.4, 5.4, 3.9, 1.3, 0.4, 5.0, 3.2, 1.2, 0.2, 5.0, 3.2, 1.2, 0.2, 4.6, 3.6, 1.0, 0.2, 4.6, 3.6, 1.0, 0.2, 7.7, 3.0, 6.1, 2.3, 7.7, 3.0, 6.1, 2.3, 7.2, 3.0, 5.8, 1.6, 7.2, 3.0, 5.8, 1.6, 5.1, 3.3, 1.7, 0.5, 5.1, 3.3, 1.7, 0.5, 6.3, 3.4, 5.6, 2.4, 6.3, 3.4, 5.6, 2.4, 5.2, 3.5, 1.5, 0.2, 5.2, 3.5, 1.5, 0.2, 4.8, 3.4, 1.6, 0.2, 4.8, 3.4, 1.6, 0.2, 7.9, 3.8, 6.4, 2.0, 7.9, 3.8, 6.4, 2.0, 6.0, 3.0, 4.8, 1.8, 6.0, 3.0, 4.8, 1.8]\n",
      "Probability Densities : [0.005646178656510727, 1.074843570254815, 1.852176626233184e-129, 1.3909655872668325e-81, 0.5555247886056214, 0.38949788326705226, 0.6774330188161242, 0.8056167619447863, 2.4400611094393817e-05, 0.702835142857856, 5.464833198970314e-143, 1.1366545984180592e-89, 0.6155099079246938, 1.085399229014555, 0.6931333307585474, 0.5012800545840629, 1.6194656318790578e-06, 0.8715601490249888, 5.245446596168693e-150, 1.3909655872668325e-81, 0.5359414919217612, 0.8554331195198305, 0.6697167323145696, 0.8056167619447863, 0.5530819559510389, 0.8715601490249888, 2.1599420706612493, 3.520567398865866, 0.005393552091976661, 0.8554331195198305, 3.071741846320037e-12, 3.436605016592245e-10, 6.5309773373778725e-06, 0.5265782805540331, 3.862574661185473e-136, 1.4796014224466848e-66, 0.5817584032778095, 1.2417521937718328, 0.6957852006270738, 1.3814698357066542, 1.0793163164358766, 1.0689299990210455, 2.1599420706612493, 3.1532363307814495, 0.046455655398917946, 0.22502335267874327, 3.071741846320037e-12, 3.7606604733561525e-09, 0.384738720051639, 0.008896590212748258, 1.2793064122397317, 3.1532363307814495, 0.003246709492237681, 0.17552069944966123, 8.547113242635482e-13, 3.7606604733561525e-09, 0.00027082828020813683, 0.5265782805540331, 1.3298725762880007e-116, 1.2861310476092008e-59, 0.6380006337229956, 1.2417521937718328, 0.5859317317569864, 1.4740297555704223, 1.100131806918123, 1.0041408360121633, 2.1599420706612493, 3.520567398865866, 0.03178861828718153, 0.6078894296975508, 3.071741846320037e-12, 3.436605016592245e-10, 0.6444026521643043, 0.5010706514567386, 1.2793064122397317, 1.1696374100325508, 0.12432028686602707, 0.008903019794187804, 8.547113242635482e-13, 3.590080797801255e-08, 1.100131806918123, 0.8715601490249888, 0.5140585482895328, 3.520567398865866, 0.03178861828718153, 0.8554331195198305, 2.3066779916718695e-13, 3.436605016592245e-10, 0.5530819559510389, 0.9876581167173136, 0.025918222260629262, 3.520567398865866, 0.005393552091976661, 0.11721739333870493, 1.532922891989058e-14, 3.436605016592245e-10, 1.4787654141362617e-12, 0.5265782805540331, 9.197293861951125e-180, 1.3909655872668325e-81, 0.11049195486516598, 1.2417521937718328, 0.4300473883959119, 0.8056167619447863, 1.5611654303256863e-08, 0.5265782805540331, 3.415808891091967e-157, 1.0898227025742455e-35, 0.35928916437305075, 1.2417521937718328, 0.6276221840159025, 0.48777725255035415, 1.0793163164358766, 1.0041408360121633, 1.0136263645514672, 0.17967880439550718, 0.046455655398917946, 0.6078894296975508, 1.187072112678628e-10, 2.9898535214736344e-07, 0.0022143775129580554, 1.074843570254815, 5.464833198970314e-143, 1.1366545984180592e-89, 0.5968642494075507, 0.38949788326705226, 0.6931333307585474, 0.5012800545840629, 0.9809996429140855, 1.0689299990210455, 2.4740881326075383, 3.520567398865866, 0.0661717515129419, 0.22502335267874327, 1.070736827373216e-11, 3.436605016592245e-10, 0.9088347718857043, 1.074843570254815, 1.9226213659306524, 3.520567398865866, 0.013782851190504425, 0.38949788326705226, 3.620041391960654e-11, 3.436605016592245e-10, 2.129080224382951e-14, 0.6762098420155263, 7.540087430013644e-204, 1.2861310476092008e-59, 0.05761757328731952, 0.023315618109827956, 0.22383565837745867, 1.4740297555704223, 0.029188298512359614, 0.5265782805540331, 6.518260610188412e-93, 6.902696786082644e-47, 0.4456157077505356, 1.2417521937718328, 0.3038096563188134, 1.1141732144080487]\n",
      "\n",
      "Testing Gaussian Probability Density for Setosa\n",
      "Test Data Points : [5.2, 2.7, 3.9, 1.4, 5.2, 2.7, 3.9, 1.4, 6.2, 2.8, 4.8, 1.8, 6.2, 2.8, 4.8, 1.8, 6.4, 2.7, 5.3, 1.9, 6.4, 2.7, 5.3, 1.9, 5.5, 2.5, 4.0, 1.3, 5.5, 2.5, 4.0, 1.3, 5.8, 2.7, 5.1, 1.9, 5.8, 2.7, 5.1, 1.9, 5.7, 2.6, 3.5, 1.0, 5.7, 2.6, 3.5, 1.0, 6.3, 2.5, 5.0, 1.9, 6.3, 2.5, 5.0, 1.9, 5.5, 2.6, 4.4, 1.2, 5.5, 2.6, 4.4, 1.2, 6.9, 3.1, 5.4, 2.1, 6.9, 3.1, 5.4, 2.1, 6.7, 3.3, 5.7, 2.1, 6.7, 3.3, 5.7, 2.1, 5.6, 3.0, 4.5, 1.5, 5.6, 3.0, 4.5, 1.5, 5.8, 2.8, 5.1, 2.4, 5.8, 2.8, 5.1, 2.4, 6.4, 2.8, 5.6, 2.1, 6.4, 2.8, 5.6, 2.1, 6.4, 3.1, 5.5, 1.8, 6.4, 3.1, 5.5, 1.8, 6.8, 2.8, 4.8, 1.4, 6.8, 2.8, 4.8, 1.4, 5.8, 2.7, 4.1, 1.0, 5.8, 2.7, 4.1, 1.0, 4.9, 2.5, 4.5, 1.7, 4.9, 2.5, 4.5, 1.7, 6.1, 2.9, 4.7, 1.4, 6.1, 2.9, 4.7, 1.4, 5.6, 2.5, 3.9, 1.1, 5.6, 2.5, 3.9, 1.1, 6.7, 3.1, 4.4, 1.4, 6.7, 3.1, 4.4, 1.4]\n",
      "Probability Densities : [0.26039634499656805, 1.1667380178959097, 0.6163113264798487, 1.9200878091892002, 0.03295781786396121, 0.7789374241993597, 0.004895439316576926, 0.1143580583866047, 0.6972335038955182, 1.1960732952302917, 0.4522377123928184, 0.1370755576234969, 0.4738155723014163, 0.9968596932898627, 0.22616214271920082, 0.9807293584710367, 0.5365253017000585, 1.1667380178959097, 0.08456409433156417, 0.03756662584174808, 0.5845453193193983, 0.7789374241993597, 0.5989457178771465, 1.2368050614260482, 0.5228504875369706, 0.8467515008068792, 0.7072098733768876, 1.9694447093831124, 0.09728253244520992, 0.3577885249649408, 0.008552673585633962, 0.04924675842832239, 0.7440765464460909, 1.1667380178959097, 0.18813613470384064, 0.03756662584174808, 0.2253566077272941, 0.7789374241993597, 0.44793406580927486, 1.2368050614260482, 0.6873041648975607, 1.0398549013008, 0.23125677568720193, 0.4635000074014243, 0.17496530178348496, 0.5535625406726307, 0.0003777217000727862, 0.001890365670220236, 0.6234331968568332, 0.8467515008068792, 0.2630924418677613, 0.03756662584174808, 0.5334091734119032, 0.3577885249649408, 0.36865538439365303, 1.2368050614260482, 0.5228504875369706, 1.0398549013008, 0.7976786358317006, 1.5672542981819781, 0.09728253244520992, 0.5535625406726307, 0.05727485116605936, 0.018769863571657452, 0.14269893527475216, 0.7495733831299181, 0.053153998720872994, 0.0013176663278833614, 0.6168862266637963, 1.1825022460550303, 0.6591243037144597, 1.3637136302458128, 0.2718458459121337, 0.3494922819281786, 0.010199008392105455, 0.0013176663278833614, 0.654531645656, 0.8245656068210239, 0.7205756964736317, 1.3637136302458128, 0.6110390177007584, 0.9586900005401613, 0.7382774731692701, 1.4523502432690931, 0.1322330820599221, 1.2282414180807255, 0.0848368977715241, 0.23503280722646236, 0.7440765464460909, 1.1960732952302917, 0.18813613470384064, 1.29005326390209e-06, 0.2253566077272941, 0.9968596932898627, 0.44793406580927486, 0.6319031246589567, 0.5365253017000585, 1.1960732952302917, 0.01845958423475675, 0.0013176663278833614, 0.5845453193193983, 0.9968596932898627, 0.7229585504448853, 1.3637136302458128, 0.5365253017000585, 0.7495733831299181, 0.0320047603334595, 0.1370755576234969, 0.5845453193193983, 1.1825022460550303, 0.7017934388467366, 0.9807293584710367, 0.2007601045592885, 1.1960732952302917, 0.4522377123928184, 1.9200878091892002, 0.6440425827595443, 0.9968596932898627, 0.22616214271920082, 0.1143580583866047, 0.7440765464460909, 1.1667380178959097, 0.7773653513508656, 0.4635000074014243, 0.2253566077272941, 0.7789374241993597, 0.014456868815967115, 0.001890365670220236, 0.09191570356361682, 0.8467515008068792, 0.7382774731692701, 0.38805280524493846, 0.008762762519320294, 0.3577885249649408, 0.0848368977715241, 0.6882871940883862, 0.7505077460884608, 1.1202786946288388, 0.5558902088466483, 1.9200878091892002, 0.409698842099012, 1.1602754208767672, 0.1685826008374964, 0.1143580583866047, 0.6110390177007584, 0.8467515008068792, 0.6163113264798487, 0.9676273170646712, 0.1322330820599221, 0.3577885249649408, 0.004895439316576926, 0.006331653987416545, 0.2718458459121337, 0.7495733831299181, 0.7976786358317006, 1.9200878091892002, 0.654531645656, 1.1825022460550303, 0.05727485116605936, 0.1143580583866047]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data_setVsVer = data[data[:, -1] != 2]\n",
    "filtered_data_setVsVir = data[data[:, -1] != 1]\n",
    "filtered_data_verVsVir = data[data[:, -1] != 0]\n",
    "# print(f\"Filtered Data : {filtered_data}\")\n",
    "prediction_class_setVsVer =['0', '1']\n",
    "prediction_class_setVsVir =['0', '2']\n",
    "prediction_class_verVsVir =['1', '2']\n",
    "\n",
    "# print(f\"Train Features : {train_features}\\nTrain Labels : {train_labels}\")\n",
    "feature_stats_setVsVer = calculate_feature_statistics(train_features_setVsVer, train_labels_setVsVer)\n",
    "feature_stats_setVsVir = calculate_feature_statistics(train_features_setVsVir, train_labels_setVsVir)\n",
    "feature_stats_verVsVir = calculate_feature_statistics(train_features_verVsVir, train_labels_verVsVir)\n",
    "\n",
    "setVsVer_X = []\n",
    "setVsVer_probDen = []\n",
    "for test_sample in test_data_setVsVer:\n",
    "    probabilities, exes, prob_dens = naive_bayes_prediction(feature_stats_setVsVer, test_sample[:-1])\n",
    "    for item in exes:\n",
    "        setVsVer_X.append(item)\n",
    "    for item in prob_dens:\n",
    "        setVsVer_probDen.append(item)\n",
    "    predicted_class = probabilities_to_prediction(probabilities)\n",
    "    predictions_setVsVer.append(predicted_class)\n",
    "    true_labels_setVsVer.append(test_sample[-1])\n",
    "print(f\"Testing Gaussian Probability Density for Versicolor\")\n",
    "print(f\"Test Data Points : {setVsVer_X}\")\n",
    "print(f\"Probability Densities : {setVsVer_probDen}\\n\")\n",
    "\n",
    "setVsVir_X = []\n",
    "setVsVir_probDen = []\n",
    "for test_sample in test_data_setVsVir:\n",
    "    probabilities, exes, prob_dens = naive_bayes_prediction(feature_stats_setVsVir, test_sample[:-1])\n",
    "    for item in exes:\n",
    "        setVsVir_X.append(item)\n",
    "    for item in prob_dens:\n",
    "        setVsVir_probDen.append(item)\n",
    "    predicted_class = probabilities_to_prediction(probabilities)\n",
    "    predictions_setVsVir.append(predicted_class)\n",
    "    true_labels_setVsVir.append(test_sample[-1])\n",
    "print(f\"Testing Gaussian Probability Density for Virginica\")\n",
    "print(f\"Test Data Points : {setVsVir_X}\")\n",
    "print(f\"Probability Densities : {setVsVir_probDen}\\n\")\n",
    "\n",
    "verVsVir_X = []\n",
    "verVsVir_probDen = []\n",
    "for test_sample in test_data_verVsVir:\n",
    "    probabilities, exes, prob_dens = naive_bayes_prediction(feature_stats_verVsVir, test_sample[:-1])\n",
    "    for item in exes:\n",
    "        verVsVir_X.append(item)\n",
    "    for item in prob_dens:\n",
    "        verVsVir_probDen.append(item)\n",
    "    predicted_class = probabilities_to_prediction(probabilities)\n",
    "    predictions_verVsVir.append(predicted_class)\n",
    "    true_labels_verVsVir.append(test_sample[-1])\n",
    "print(f\"Testing Gaussian Probability Density for Setosa\")\n",
    "print(f\"Test Data Points : {verVsVir_X}\")\n",
    "print(f\"Probability Densities : {verVsVir_probDen}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iris-Setosa vs Iris-Versicolor\n",
      "True labels: [0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "Predictions: [0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "Accuracy: 100.00%\n",
      "\n",
      "For Iris-Setosa vs Iris-Virginica\n",
      "True labels: [2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 2]\n",
      "Predictions: [2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 2]\n",
      "Accuracy: 100.00% \n",
      "\n",
      "For Iris-Versicolor vs Iris-Virginica\n",
      "True labels: [1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1]\n",
      "Predictions: [1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1]\n",
      "Accuracy: 95.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_setVsVer = sum(p == t for p, t in zip(predictions_setVsVer, true_labels_setVsVer)) / len(true_labels_setVsVer)\n",
    "print(\"For Iris-Setosa vs Iris-Versicolor\")\n",
    "print(f\"True labels: {true_labels_setVsVer}\")\n",
    "print(f\"Predictions: {predictions_setVsVer}\")\n",
    "print(f\"Accuracy: {accuracy_setVsVer * 100:.2f}%\\n\")\n",
    "\n",
    "accuracy_setVsVir = sum(p == t for p, t in zip(predictions_setVsVir, true_labels_setVsVir)) / len(true_labels_setVsVir)\n",
    "print(\"For Iris-Setosa vs Iris-Virginica\")\n",
    "print(f\"True labels: {true_labels_setVsVir}\")\n",
    "print(f\"Predictions: {predictions_setVsVir}\")\n",
    "print(f\"Accuracy: {accuracy_setVsVir * 100:.2f}% \\n\")\n",
    "\n",
    "accuracy_verVsVir = sum(p == t for p, t in zip(predictions_verVsVir, true_labels_verVsVir)) / len(true_labels_verVsVir)\n",
    "print(\"For Iris-Versicolor vs Iris-Virginica\")\n",
    "print(f\"True labels: {true_labels_verVsVir}\")\n",
    "print(f\"Predictions: {predictions_verVsVir}\")\n",
    "print(f\"Accuracy: {accuracy_verVsVir * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_setVsVer = filtered_data_setVsVer[:, -1]\n",
    "data_setVsVer = filtered_data_setVsVer[:, :-1] \n",
    "\n",
    "target_setVsVir = filtered_data_setVsVir[:, -1]\n",
    "data_setVsVir = filtered_data_setVsVir[:, :-1] \n",
    "\n",
    "target_verVsVir = filtered_data_verVsVir[:, -1]\n",
    "data_verVsVir = filtered_data_verVsVir[:, :-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Unique Classes Debug : [0 1] : 2\n",
      "TPR_FPR Unique Classes Debug : [0] : 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The labels must be binary (two unique classes).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_setVsVer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_setVsVer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_stepsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cross_validation(data_setVsVir, target_setVsVir, folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, threshold_stepsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      3\u001b[0m cross_validation(data_verVsVir, target_verVsVir, folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, threshold_stepsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[140], line 28\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(data, target, folds, threshold_stepsize)\u001b[0m\n\u001b[0;32m     25\u001b[0m     prediction_probabilities\u001b[38;5;241m.\u001b[39mappend(prob_temp[class_1])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate TPR and FPR\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m TPR, FPR \u001b[38;5;241m=\u001b[39m \u001b[43mTPR_and_FPR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_stepsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample True Positive Rate : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTPR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[138], line 13\u001b[0m, in \u001b[0;36mTPR_and_FPR\u001b[1;34m(prediction_probabilities, test_labels, threshold_stepsize)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPR_FPR Unique Classes Debug : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_labels) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe labels must be binary (two unique classes).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m positive_class \u001b[38;5;241m=\u001b[39m unique_labels[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Higher value as positive class\u001b[39;00m\n\u001b[0;32m     15\u001b[0m true_binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m positive_class \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m test_labels])\n",
      "\u001b[1;31mValueError\u001b[0m: The labels must be binary (two unique classes)."
     ]
    }
   ],
   "source": [
    "cross_validation(data_setVsVer, target_setVsVer, folds=10, threshold_stepsize=0.1)\n",
    "cross_validation(data_setVsVir, target_setVsVir, folds=10, threshold_stepsize=0.1)\n",
    "cross_validation(data_verVsVir, target_verVsVir, folds=10, threshold_stepsize=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for examination:\n",
    "\n",
    "In addition to completing the assignment with all its tasks, you should also prepare to answer the following questions:\n",
    "\n",
    "1) Why is it called \"naive bayes\"?\n",
    "<br>Ans : The Naive Bayes algorithm is an implementation of the traditional Bayes Theorem which was named after `Reverend Thomas Bayes` who studied Binomial Distributions and their probabilities and hence the model that implemented this algorithm was also named Naive Bayes after `Reverend Thomas Bayes`. The term `Naive`came into the name because of the independence assumtions made to reach the theorem which at that time was considered a 'Naive' approach.\n",
    "\n",
    "2) What are some downsides of the naive bayes learning algorithm?\n",
    "<br>Ans : The Naive Bayes algorithm has the following demerits to it : \n",
    "    - Conditional Independence Assumption : The assumptions about independence made in the algorithm doesn't always hold in real life situations\n",
    "    - Zero Probability Problem : Any words encountered in the test data that aren't in the training data results in class probability value of zero\n",
    "\n",
    "3) When using ROC-curves, what is the theoretical best and worst result you can get?\n",
    "<br>Ans : Theoretically, the best ROC curve would have a curve score of 1.0. This is the case where the predictions exactly match with actual values for all tested values within and outside the testing data. The worst case would be a curve score of 0.5. This would be the equivalent of randomly guessing the outcome.\n",
    "\n",
    "4) When using ROC-curves, in this assignment for example, is a higher threshold-stepsize always better?  \n",
    "<br>Ans : No, a smaller threshold step size would provide a smoother ROC curve but it could possibly increase the computational time, but inversely, a larger threshold step size would reduce precision but can complete computations faster. This is usually dynamic and can vary based on the dataset used.\n",
    "\n",
    "5) When using cross-validation and ROC-curves, why is it important to take the correct mean values? What could go wrong?\n",
    "<br>Ans : Compared to numeric representations, ROC curves can represent the average performance of the model over each folds of cross validation in a more visually pleasing and understandable method and hence is helpful for a human analyst.\n",
    "This can be offset by incorrect mean values since incorrect mean values can grossly misrepresent the true performance of the model, often leading to wrongly reached conclusions about the model's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
